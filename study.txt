# ============================================================
# lm_ac: Linear Model using Available Cases (pairwise deletion)
# ------------------------------------------------------------
# Implements regression when X or y contain NAs by computing
# each element of X'X and X'y as the *average* of intact pairs,
# then solving the normal equations directly on those averages
# ============================================================

# -----------------------------------------------------------------------
# Constructor: Initializes an lm_ac object with formula and raw data.
# -----------------------------------------------------------------------
lm_ac <- function(data, yName, holdout = NULL, ...) {
  # ----------------------------
  # Input validation
  # ----------------------------
  if (!is.data.frame(data)) stop("Input 'data' must be a data.frame.")
  if (!(yName %in% names(data))) stop(paste("Column", yName, "not found in data."))
  if (!is.numeric(data[[yName]])) stop("Response variable must be numeric.")
  if (nrow(data) == 0) stop("Input data is empty.")
  
  # ----------------------------
  # Holdout set for bootstrap or validation
  # ----------------------------
  if (!is.null(holdout)) {
    if (is.logical(holdout) && length(holdout) == nrow(data)) {
      training_data <- data[!holdout, , drop = FALSE]
      testing_data  <- data[ holdout, , drop = FALSE]
    } else if (is.numeric(holdout) && all(holdout %in% seq_len(nrow(data)))) {
      training_data <- data[-holdout, , drop = FALSE]
      testing_data  <- data[ holdout, , drop = FALSE]
    } else {
      stop("Holdout is invalid")
    }
  } else {
    training_data <- data
    testing_data  <- NULL
  }
  if (nrow(training_data) == 0) stop("No training rows available.")
  
  # ----------------------------
  # Convert character/logical predictors to factors
  # ----------------------------
  for (col in names(training_data)) {
    if (is.character(training_data[[col]]) || is.logical(training_data[[col]])) {
      training_data[[col]] <- as.factor(training_data[[col]])
    }
  }
  
  # ----------------------------
  # Construct regression formula: y ~ all other predictors
  # ----------------------------
  formula <- reformulate(setdiff(names(training_data), yName), response = yName)
  
  # ----------------------------
  # Build model.frame and model.matrix with na.pass
  #   (this ensures rows with NAs are retained for AC math)
  # ----------------------------
  mf <- model.frame(formula, training_data, na.action = na.pass, ...)
  y  <- model.response(mf)
  X  <- model.matrix(attr(mf, "terms"), mf, na.action = na.pass)
  
  # store the terms for consistent newdata handling
  trm  <- terms(mf)
  coln <- colnames(X)
  
  # ----------------------------
  # Compute average-based XtX and Xty (pairwise deletion)
  # ----------------------------
  XtX_avg <- ac_mul(X)
  Xty_avg <- ac_vec(X, y)
  
  if (anyNA(XtX_avg) || anyNA(Xty_avg)) {
    stop("
      Missing values in system matrices: 
      need at least one intact pair per term (and intercept).
    ")
  }
  
  # ----------------------------
  # Solve system and catch singular matrix errors
  # ----------------------------
  beta_hat <- tryCatch(
    solve(XtX_avg, Xty_avg),
    error = function(e) {
      # tiny ridge to handle singularity
      lam <- 1e-6
      solve(XtX_avg + diag(lam, ncol(XtX_avg)), Xty_avg)
    }
  )
  
  # ----------------------------
  # Return lm_ac object
  # ----------------------------
  obj <- list(
    data         = training_data,
    testing_data = testing_data,  # raw holdout (predict() will handle transform)
    yName        = yName,
    formula      = formula,
    terms        = trm,
    fit_obj      = list(
      coef     = as.vector(beta_hat),
      colnames = coln
    )
  )
  class(obj) <- "lm_ac"
  obj
}

# -----------------------------------------------------------------------
# ac_mul: Compute (1/n_ij) * sum[X_i * X_j] over available pairs
# For each pair of columns (i,j), only rows where both are non-NA are used.
# -----------------------------------------------------------------------
ac_mul <- function(A) {
  if (!is.matrix(A)) A <- as.matrix(A)
  p <- ncol(A)
  result <- matrix(NA_real_, p, p)
  for (i in 1:p) {
    xi <- A[, i]
    for (j in i:p) {
      xj <- A[, j]
      valid_idx <- !is.na(xi) & !is.na(xj)
      if (any(valid_idx)) {
        val <- mean(xi[valid_idx] * xj[valid_idx])
        result[i, j] <- val
        result[j, i] <- val
      }
    }
  }
  colnames(result) <- colnames(A)
  rownames(result) <- colnames(A)
  result
}

# -----------------------------------------------------------------------
# ac_vec: Compute (1/n_i) * sum[X_i * y] over available pairs
# -----------------------------------------------------------------------
ac_vec <- function(X, y) {
  if (!is.matrix(X)) X <- as.matrix(X)
  p <- ncol(X)
  result <- numeric(p)
  for (i in 1:p) {
    xi <- X[, i]
    valid_idx <- !is.na(xi) & !is.na(y)
    result[i] <- if (any(valid_idx)) mean(xi[valid_idx] * y[valid_idx]) else NA_real_
  }
  names(result) <- colnames(X)
  result
}

# -----------------------------------------------------------------------
# summary.lm_ac: Print coefficients (same shape as lm())
# -----------------------------------------------------------------------
summary.lm_ac <- function(object, ...) {
  if (is.null(object$fit_obj)) stop("Model not fitted.")
  coefs <- object$fit_obj$coef
  names(coefs) <- object$fit_obj$colnames
  print(coefs)
  invisible(coefs)
}

# -----------------------------------------------------------------------
# predict.lm_ac: Given newdata (with possible NAs), rebuild design matrix
# using stored terms, and compute y_hat = X_new %*% Î²_hat (na.pass here).
# NOTE: In evaluation, we clean test rows to ensure no NAs (see test func).
# -----------------------------------------------------------------------
predict.lm_ac <- function(object, newdata, ...) {
  stopifnot(inherits(object, "lm_ac"))
  if (missing(newdata)) stop("'newdata' is required")
  
  newdata <- as.data.frame(newdata)
  for (nm in names(newdata)) {
    if (is.character(newdata[[nm]]) || is.logical(newdata[[nm]])) {
      newdata[[nm]] <- as.factor(newdata[[nm]])
    }
  }
  
  mf_new <- model.frame(object$terms, newdata, na.action = na.pass, ...)
  if (nrow(mf_new) == 0L) return(numeric(0))
  X_new  <- model.matrix(object$terms, mf_new, na.action = na.pass)
  
  # align to training columns
  train_cols <- object$fit_obj$colnames
  miss <- setdiff(train_cols, colnames(X_new))
  if (length(miss)) {
    X_new <- cbind(X_new, matrix(0, nrow(X_new), length(miss),
                                 dimnames = list(NULL, miss)))
  }
  X_new <- X_new[, train_cols, drop = FALSE]
  
  as.vector(X_new %*% object$fit_obj$coef)
}


# -----------------------------------------------------------------------
# bootstrap (optional): NA-free test via model terms
# -----------------------------------------------------------------------
bootstrap <- function(
    data, yName, coef_name, reps = 100, holdout_size = 0.2, ...
) {
  n <- nrow(data)
  if (n < 2) stop("Not enough rows for bootstrap.")
  if (holdout_size <= 0 || holdout_size >= 1) stop("'holdout_size' should be in (0,1).")
  
  coefs <- numeric(reps)
  mspe_values <- numeric(reps)
  mape_values <- numeric(reps)
  size <- max(1L, floor(holdout_size * n))
  
  for (i in seq_len(reps)) {
    holdout_set <- sample.int(n, size = size)
    obj <- lm_ac(data, yName, holdout = holdout_set, ...)
    
    beta <- obj$fit_obj$coef
    names(beta) <- obj$fit_obj$colnames
    coefs[i] <- if (coef_name %in% names(beta)) beta[[coef_name]] else NA_real_
    
    # Clean test (no NAs w.r.t. model terms)
    te_raw <- obj$testing_data
    mf_te  <- model.frame(obj$terms, te_raw, na.action = na.omit)
    if (nrow(mf_te) == 0L) {
      mspe_values[i] <- NA_real_
      mape_values[i] <- NA_real_
      next
    }
    y_true <- model.response(mf_te)
    X_te   <- model.matrix(obj$terms, mf_te, na.action = na.pass)
    X_te   <- X_te[, obj$fit_obj$colnames, drop = FALSE]
    y_pred <- as.numeric(X_te %*% obj$fit_obj$coef)
    mspe_values[i] <- mean((y_true - y_pred)^2)
    mape_values[i] <- mean(abs((y_true - y_pred) / y_true))
  }
  
  avg_coef <- mean(coefs, na.rm = TRUE)
  se <- sd(coefs, na.rm = TRUE) / sqrt(sum(!is.na(coefs)))
  CI <- c(avg_coef - (1.96 * se), avg_coef + (1.96 * se))
  
  list(
    coef_mean = avg_coef,
    coef_se   = se,
    coef_CI   = CI,
    coefs     = coefs,
    mspe      = mean(mspe_values, na.rm = TRUE),
    mape      = mean(mape_values, na.rm = TRUE)
  )
}
set.seed(42)

# Load Adult (treat '?' as NA)
url <- "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
cols <- c(
  "age","workclass","fnlwgt","education","education_num","marital_status",
  "occupation","relationship","race","sex","capital_gain","capital_loss",
  "hours_per_week","native_country","income"
)
adult <- read.table(url, sep=",", header=FALSE, col.names=cols,
                    na.strings="?", strip.white=TRUE)

# --- % missing per column (on raw 'adult') ---
miss <- data.frame(
  column = names(adult),
  pct_missing = round(colMeans(is.na(adult)) * 100, 2)
)
miss <- miss[order(-miss$pct_missing, miss$column), ]
print(miss, row.names = FALSE)

# Make numeric target and drop label from X
adult$income_num <- ifelse(adult$income == ">50K", 1, 0)
df <- subset(adult, select = -c(income, education, fnlwgt, native_country))

# 2) Keep only complete cases for baseline comparison
df_complete <- na.omit(df)

# 3) 80/20 split
n <- nrow(df_complete)
test_idx <- sample.int(n, size = round(0.2 * n))
train <- df_complete[-test_idx, ]
test  <- df_complete[ test_idx, ]

# 4) Fit a normal linear model (OLS)
fit_lm <- lm(income_num ~ ., data = train)

# 5) Predict on the test set
y_true <- test$income_num
y_hat  <- predict(fit_lm, newdata = test)

# 6) Convert to binary (0/1) and compute accuracy
pred <- as.integer(y_hat >= 0.5)
acc_cc  <- mean(pred == y_true)

cat("\nBaseline lm() with complete cases:\n")
cat("  Train rows:", nrow(train), "\n")
cat("  Test  rows:", nrow(test),  "\n")
cat("  Accuracy  :", round(acc_cc, 3), "\n")


# 80/20 split
n <- nrow(df)
test_idx <- sample.int(n, size = round(0.2 * n))

# 4) Fit lm_ac with income_num as y
fit <- lm_ac(df, yName = "income_num", holdout = test_idx)

cat("\nCoefficients:\n")
co <- fit$fit_obj$coef
names(co) <- fit$fit_obj$colnames
print(co)

# 5) Predict using the method
te_raw <- fit$testing_data

# Build clean test
mf_te  <- model.frame(fit$terms, fit$testing_data, na.action = na.omit)
y_true <- model.response(mf_te)
y_hat  <- predict.lm_ac(fit, newdata = mf_te)
pred   <- as.integer(y_hat >= 0.5)

acc_ac   <- mean(pred == y_true)
cat("Accuracy:", round(acc_ac, 3), "\n")
cat("Acc_CC: ", round(acc_cc, 3), "Acc_AC: ", round(acc_ac, 3), "\n")


# 6) Simple metrics
acc <- mean(pred == y_true)
tp  <- sum(pred == 1 & y_true == 1)
tn  <- sum(pred == 0 & y_true == 0)
fp  <- sum(pred == 1 & y_true == 0)
fn  <- sum(pred == 0 & y_true == 1)

cat("\nTest results (Linear Probability Model via lm_ac):\n")
cat("  Rows used:", length(y_true), "of", length(test_idx), "\n")
cat("  Accuracy :", round(acc, 3), "\n")
cat("  Confusion matrix:\n")
print(matrix(c(tp, fp, fn, tn), nrow = 2, byrow = TRUE,
             dimnames = list("Pred=1/0" = c("Pred=1","Pred=0"),
                             "True=1/0" = c("True=1","True=0"))))

